# Statistical Equations and Power Analysis Library

## ğŸ§® Comprehensive Mathematical Framework for QA System Agent

This document provides complete statistical equations, power analysis formulas, and computational implementations for all experiment types, with academic validation and practical implementation guidance.

---

## ğŸ“Š Experiment Type 1: Comparison Studies

### **1.1 Paired t-test (Within-Subjects Comparison)**

#### **Core Statistical Model**
```yaml
mathematical_foundation:
  population_model: "d_i = Xâ‚áµ¢ - Xâ‚‚áµ¢ ~ N(Î¼_d, ÏƒÂ²_d)"
  null_hypothesis: "Hâ‚€: Î¼_d = 0"
  alternative_hypothesis: "Hâ‚: Î¼_d â‰  0 (two-tailed) or Î¼_d > 0 (one-tailed)"
  
test_statistic:
  formula: "t = (dÌ„ - Î¼â‚€) / (s_d / âˆšn)"
  degrees_of_freedom: "df = n - 1"
  where:
    dÌ„: "sample mean of difference scores"
    s_d: "sample standard deviation of differences"
    n: "number of paired observations"
    Î¼â‚€: "hypothesized mean difference (usually 0)"
```

#### **Effect Size Calculations**
```yaml
cohens_d_paired:
  formula: "d = dÌ„ / s_d"
  interpretation: "Cohen (1988): |d| = 0.2 (small), 0.5 (medium), 0.8 (large)"
  
  bias_correction: "Hedges' g = d Ã— (1 - 3/(4df - 1))"
  confidence_interval: "d Â± t_Î±/2,df Ã— âˆš((n+1)/(n) + dÂ²/(2n))"
  
glass_delta:
  formula: "Î” = dÌ„ / s_control"
  application: "When using control group SD as standardizer"
  
correlation_adjustment:
  repeated_measures_d: "d_rm = d_between Ã— âˆš(2(1-r))"
  where: "r = correlation between repeated measures"
```

#### **Power Analysis Framework**
```yaml
sample_size_calculation:
  two_tailed: "n = 2(z_Î±/2 + z_Î²)Â² / dÂ²"
  one_tailed: "n = (z_Î± + z_Î²)Â² / dÂ²"
  
  exact_formula: "n = (t_Î±/2,df + t_Î²,df)Â² / dÂ²"
  iterative_solution: "Requires iterative solving since df = n-1"
  
power_calculation:
  noncentrality_parameter: "Î´ = dâˆšn"
  power: "1 - Î² = P(t > t_Î±/2,df | Î´) + P(t < -t_Î±/2,df | Î´)"
  
minimum_detectable_effect:
  formula: "MDE = (t_Î±/2,df + t_Î²,df) / âˆšn"
  practical_application: "Smallest effect size detectable with given n and power"

confidence_interval_precision:
  margin_of_error: "ME = t_Î±/2,df Ã— (s_d / âˆšn)"
  required_n_for_precision: "n = (t_Î±/2 Ã— s_d / ME)Â²"
  planning_values: "Use pilot data or literature estimates for s_d"
```

#### **Implementation Code Templates**
```python
# Python implementation for paired t-test power analysis
import numpy as np
from scipy import stats
from scipy.optimize import fsolve

def paired_ttest_power(n=None, effect_size=None, alpha=0.05, power=None, alternative='two-sided'):
    """
    Power analysis for paired t-test
    Provide three of four parameters, calculates the fourth
    """
    
    if alternative == 'two-sided':
        z_alpha = stats.norm.ppf(1 - alpha/2)
    else:
        z_alpha = stats.norm.ppf(1 - alpha)
    
    if power is not None:
        z_beta = stats.norm.ppf(power)
    
    if n is None:  # Calculate required sample size
        if alternative == 'two-sided':
            n = 2 * (z_alpha + z_beta)**2 / effect_size**2
        else:
            n = (z_alpha + z_beta)**2 / effect_size**2
        return int(np.ceil(n))
    
    elif effect_size is None:  # Calculate minimum detectable effect
        if alternative == 'two-sided':
            effect_size = (z_alpha + z_beta) * np.sqrt(2/n)
        else:
            effect_size = (z_alpha + z_beta) * np.sqrt(1/n)
        return effect_size
    
    elif power is None:  # Calculate achieved power
        if alternative == 'two-sided':
            ncp = effect_size * np.sqrt(n/2)
        else:
            ncp = effect_size * np.sqrt(n)
        power = 1 - stats.norm.cdf(z_alpha - ncp)
        if alternative == 'two-sided':
            power += stats.norm.cdf(-z_alpha - ncp)
        return power

def cohens_d_ci(d, n, alpha=0.05):
    """Calculate confidence interval for Cohen's d"""
    df = n - 1
    t_crit = stats.t.ppf(1 - alpha/2, df)
    
    se_d = np.sqrt((n + 1)/n + d**2/(2*n))
    ci_lower = d - t_crit * se_d
    ci_upper = d + t_crit * se_d
    
    return ci_lower, ci_upper
```

### **1.2 Independent Samples t-test (Between-Subjects Comparison)**

#### **Statistical Model**
```yaml
population_model:
  group_1: "Xâ‚áµ¢ ~ N(Î¼â‚, ÏƒÂ²)"
  group_2: "Xâ‚‚áµ¢ ~ N(Î¼â‚‚, ÏƒÂ²)"
  homoscedasticity: "Equal variances assumed (Ïƒâ‚Â² = Ïƒâ‚‚Â² = ÏƒÂ²)"
  
test_statistic:
  equal_variances: "t = (xÌ„â‚ - xÌ„â‚‚) / (s_p Ã— âˆš(1/nâ‚ + 1/nâ‚‚))"
  pooled_variance: "sÂ²_p = [(nâ‚-1)sâ‚Â² + (nâ‚‚-1)sâ‚‚Â²] / (nâ‚ + nâ‚‚ - 2)"
  degrees_of_freedom: "df = nâ‚ + nâ‚‚ - 2"
  
welch_correction:
  unequal_variances: "t = (xÌ„â‚ - xÌ„â‚‚) / âˆš(sâ‚Â²/nâ‚ + sâ‚‚Â²/nâ‚‚)"
  satterthwaite_df: "df = (sâ‚Â²/nâ‚ + sâ‚‚Â²/nâ‚‚)Â² / [(sâ‚Â²/nâ‚)Â²/(nâ‚-1) + (sâ‚‚Â²/nâ‚‚)Â²/(nâ‚‚-1)]"
```

#### **Effect Size and Power Analysis**
```yaml
cohens_d_independent:
  formula: "d = (Î¼â‚ - Î¼â‚‚) / Ïƒ_pooled"
  sample_estimate: "d = (xÌ„â‚ - xÌ„â‚‚) / s_pooled"
  
balanced_design_power:
  sample_size_per_group: "n = 2(z_Î±/2 + z_Î²)Â² / dÂ²"
  unbalanced_design: "n_h = 2(z_Î±/2 + z_Î²)Â² / [dÂ² Ã— (1 + 1/r)]"
  where: "r = nâ‚‚/nâ‚ (allocation ratio)"
  
optimal_allocation:
  cost_consideration: "nâ‚/nâ‚‚ = âˆš(câ‚‚/câ‚)"
  where: "câ‚, câ‚‚ = cost per participant in groups 1, 2"
  equal_cost_optimal: "nâ‚ = nâ‚‚ (balanced design)"
```

### **1.3 Mixed-Effects Models for Hierarchical Data**

#### **Model Specification**
```yaml
basic_random_intercept:
  model: "Y_ij = Î²â‚€ + Î²â‚X_ij + u_j + Îµ_ij"
  where:
    Y_ij: "response for participant j on trial i"
    Î²â‚€: "fixed intercept"
    Î²â‚: "fixed effect of treatment"
    u_j: "random intercept for participant j ~ N(0, ÏƒÂ²_u)"
    Îµ_ij: "residual error ~ N(0, ÏƒÂ²_Îµ)"
    
variance_components:
  total_variance: "Var(Y_ij) = ÏƒÂ²_u + ÏƒÂ²_Îµ"
  intraclass_correlation: "ICC = ÏƒÂ²_u / (ÏƒÂ²_u + ÏƒÂ²_Îµ)"
  
random_slopes_model:
  model: "Y_ij = Î²â‚€ + Î²â‚X_ij + uâ‚€j + uâ‚jX_ij + Îµ_ij"
  random_effects: "[uâ‚€j, uâ‚j] ~ N(0, G)"
  covariance_matrix: "G = [ÏƒÂ²_uâ‚€, Ïƒ_uâ‚€uâ‚; Ïƒ_uâ‚€uâ‚, ÏƒÂ²_uâ‚]"
```

#### **Power Analysis for Mixed Models**
```yaml
effective_sample_size:
  formula: "n_eff = n_participants Ã— n_observations / [1 + (n_observations - 1) Ã— ICC]"
  application: "Use n_eff in standard power calculations"
  
design_effect:
  formula: "DE = 1 + (n_observations - 1) Ã— ICC"
  sample_size_inflation: "n_required = n_simple Ã— DE"
  
power_approximation:
  method_1: "Use n_eff in t-test power formulas"
  method_2: "Monte Carlo simulation for complex designs"
  method_3: "Analytic approximations (Snijders & Bosker, 2012)"
```

---

## ğŸ” Experiment Type 2: Exploratory Analysis

### **2.1 Analysis of Variance (ANOVA)**

#### **One-Way ANOVA**
```yaml
model_specification:
  population_model: "Y_ij = Î¼ + Î±_i + Îµ_ij"
  where:
    Î¼: "grand mean"
    Î±_i: "effect of group i"
    Îµ_ij: "random error ~ N(0, ÏƒÂ²)"
    
  constraints: "Î£Î±_i = 0 (sum-to-zero constraint)"
  
hypothesis_testing:
  null_hypothesis: "Hâ‚€: Î±â‚ = Î±â‚‚ = ... = Î±_k = 0"
  alternative_hypothesis: "Hâ‚: not all Î±_i = 0"
  
f_statistic:
  formula: "F = MS_between / MS_within"
  mean_squares: "MS_between = SS_between / (k-1)"
  mean_squares: "MS_within = SS_within / (N-k)"
  
  sum_of_squares:
    between: "SS_between = Î£n_i(xÌ„_i - xÌ„)Â²"
    within: "SS_within = Î£Î£(x_ij - xÌ„_i)Â²"
    total: "SS_total = Î£Î£(x_ij - xÌ„)Â²"
```

#### **Effect Size Measures**
```yaml
eta_squared:
  formula: "Î·Â² = SS_between / SS_total"
  interpretation: "Proportion of total variance explained"
  bias: "Positively biased, especially with small samples"
  
partial_eta_squared:
  formula: "Î·Â²_p = SS_between / (SS_between + SS_within)"
  preferred: "Less biased than Î·Â²"
  interpretation: "Cohen (1988): 0.01 (small), 0.06 (medium), 0.14 (large)"
  
omega_squared:
  formula: "Ï‰Â² = (SS_between - (k-1)MS_within) / (SS_total + MS_within)"
  advantage: "Unbiased estimate of population effect size"
  
cohens_f:
  formula: "f = âˆš(Î·Â²_p / (1 - Î·Â²_p))"
  power_analysis: "Used in power calculations"
  interpretation: "0.10 (small), 0.25 (medium), 0.40 (large)"
```

#### **Power Analysis**
```yaml
sample_size_calculation:
  formula: "n = Î» / (k Ã— fÂ²) + 1"
  where:
    Î»: "noncentrality parameter from power tables"
    k: "number of groups"
    fÂ²: "Cohen's f squared"
    
  noncentrality_parameter: "Î» = n Ã— k Ã— fÂ²"
  
power_calculation:
  distribution: "F follows noncentral F-distribution under Hâ‚"
  power: "P(F > F_Î±,k-1,N-k | Î»)"
  
minimum_detectable_effect:
  formula: "f = âˆš(Î» / (n Ã— k))"
  where: "Î» = critical value for desired power"
```

### **2.2 Factorial ANOVA**

#### **Two-Way ANOVA Model**
```yaml
model_specification:
  full_model: "Y_ijk = Î¼ + Î±_i + Î²_j + (Î±Î²)_ij + Îµ_ijk"
  where:
    Î±_i: "main effect of factor A (level i)"
    Î²_j: "main effect of factor B (level j)"
    (Î±Î²)_ij: "interaction effect"
    Îµ_ijk: "random error"
    
main_effects:
  factor_a: "F_A = MS_A / MS_error"
  factor_b: "F_B = MS_B / MS_error"
  
interaction_effect:
  f_statistic: "F_AB = MS_AB / MS_error"
  interpretation: "Effect of A depends on level of B"
  
degrees_of_freedom:
  factor_a: "df_A = a - 1"
  factor_b: "df_B = b - 1"
  interaction: "df_AB = (a-1)(b-1)"
  error: "df_error = N - ab"
```

#### **Effect Sizes and Power**
```yaml
partial_eta_squared:
  main_effect_a: "Î·Â²_p,A = SS_A / (SS_A + SS_error)"
  main_effect_b: "Î·Â²_p,B = SS_B / (SS_B + SS_error)"
  interaction: "Î·Â²_p,AB = SS_AB / (SS_AB + SS_error)"
  
power_analysis_considerations:
  main_effects: "Calculated as if other factors don't exist"
  interaction_power: "Often requires larger samples than main effects"
  sample_size_per_cell: "n_cell = total_n / (a Ã— b)"
  
factorial_design_efficiency:
  advantage: "Can detect interactions"
  cost: "Requires larger total sample size"
  optimal_allocation: "Equal n per cell usually optimal"
```

### **2.3 Multivariate Analysis of Variance (MANOVA)**

#### **Model and Test Statistics**
```yaml
model_specification:
  multivariate_model: "Y = XB + E"
  where:
    Y: "n Ã— p matrix of responses"
    X: "n Ã— q design matrix"
    B: "q Ã— p parameter matrix"
    E: "n Ã— p error matrix"
    
test_statistics:
  wilks_lambda: "Î› = |E| / |H + E|"
  pillai_trace: "V = tr(H(H + E)^-1)"
  hotelling_lawley_trace: "T = tr(HE^-1)"
  roy_largest_root: "Î¸ = largest eigenvalue of HE^-1"
  
hypothesis_matrices:
  hypothesis: "H = C'(C(X'X)^-1C')^-1C'"
  error: "E = Y'Y - B'X'XB"
```

#### **Effect Sizes and Interpretation**
```yaml
multivariate_effect_sizes:
  partial_eta_squared: "Î·Â²_p = 1 - Î›^(1/s)"
  where: "s = min(p, df_hypothesis)"
  
  pillai_conversion: "Î·Â²_p = V / (s - V + 1)"
  
interpretation_guidelines:
  small_effect: "Î·Â²_p â‰ˆ 0.01"
  medium_effect: "Î·Â²_p â‰ˆ 0.06"
  large_effect: "Î·Â²_p â‰ˆ 0.14"
  
discriminant_analysis:
  canonical_variates: "Eigenvectors of E^-1H"
  separation_assessment: "Canonical correlations"
```

---

## ğŸ† Experiment Type 3: Ranking Studies

### **3.1 Kendall's Concordance**

#### **Mathematical Foundation**
```yaml
concordance_coefficient:
  formula: "W = 12S / [mÂ²(nÂ³ - n)]"
  where:
    S: "sum of squared deviations of rank sums"
    m: "number of judges/raters"
    n: "number of objects ranked"
    
computational_formula:
  rank_sums: "R_j = Î£(rank given to object j by each judge)"
  mean_rank_sum: "RÌ„ = mn(n+1)/2"
  sum_of_squares: "S = Î£(R_j - RÌ„)Â²"
  
significance_testing:
  large_sample: "Ï‡Â² = m(n-1)W ~ Ï‡Â²(n-1)"
  small_sample: "Use exact tables (Siegel & Castellan, 1988)"
  
correction_for_ties:
  tied_ranks: "W = 12S / [mÂ²(nÂ³ - n) - mÎ£T_i]"
  where: "T_i = t_iÂ³ - t_i for t_i tied observations in rank i"
```

#### **Interpretation and Effect Sizes**
```yaml
concordance_interpretation:
  perfect_agreement: "W = 1.0"
  no_agreement: "W = 0.0"
  random_rankings: "W = 0.0 expected value"
  
practical_guidelines:
  weak_agreement: "W < 0.3"
  moderate_agreement: "0.3 â‰¤ W < 0.7"
  strong_agreement: "W â‰¥ 0.7"
  
reliability_as_icc:
  relationship: "W = (ICC - 1/m) Ã— m/(m-1)"
  where: "ICC = intraclass correlation coefficient"
```

#### **Power and Sample Size**
```yaml
power_analysis:
  effect_size_parameter: "W itself serves as effect size"
  
sample_size_requirements:
  small_effect: "W = 0.1 requires m â‰¥ 20 judges"
  medium_effect: "W = 0.3 requires m â‰¥ 10 judges"  
  large_effect: "W = 0.7 requires m â‰¥ 5 judges"
  
  minimum_objects: "n â‰¥ 3 for meaningful ranking"
  practical_maximum: "n â‰¤ 10 for cognitive feasibility"
  
monte_carlo_power:
  simulation_approach: "Generate rankings under alternative hypothesis"
  power_estimation: "Proportion of simulations rejecting Hâ‚€"
```

### **3.2 Bradley-Terry Model**

#### **Model Specification**
```yaml
probability_model:
  pairwise_comparison: "P(i beats j) = Ï€_i / (Ï€_i + Ï€_j)"
  strength_parameters: "Ï€_i > 0 for all objects i"
  identifiability: "Î£log(Ï€_i) = 0 or Ï€_n = 1 (reference object)"
  
likelihood_function:
  log_likelihood: "LL = Î£Î£ w_ij log(Ï€_i / (Ï€_i + Ï€_j))"
  where: "w_ij = number of times i beats j"
  
parameter_estimation:
  iterative_scaling: "Ï€_i^(t+1) = Ï€_i^(t) Ã— w_i+ / Î£_jâ‰ i Ï€_j^(t)/(Ï€_i^(t) + Ï€_j^(t))"
  convergence_criterion: "||Ï€^(t+1) - Ï€^(t)|| < Îµ"
```

#### **Statistical Inference**
```yaml
standard_errors:
  fisher_information: "I_ij = Î£_kâ‰ i n_ik Ï€_k / (Ï€_i + Ï€_k)Â²"
  asymptotic_variance: "Var(log Ï€_i) = (I^-1)_ii"
  
confidence_intervals:
  log_scale: "log Ï€_i Â± z_Î±/2 Ã— SE(log Ï€_i)"
  strength_scale: "exp(log Ï€_i Â± z_Î±/2 Ã— SE(log Ï€_i))"
  
hypothesis_testing:
  equal_strengths: "Hâ‚€: Ï€_1 = Ï€_2 = ... = Ï€_n"
  likelihood_ratio: "LR = 2(LL_full - LL_reduced)"
  degrees_of_freedom: "df = n - 1"
  
pairwise_comparisons:
  strength_ratio: "Ï€_i / Ï€_j"
  log_ratio_se: "SE(log(Ï€_i/Ï€_j)) = âˆš(Var(log Ï€_i) + Var(log Ï€_j))"
```

#### **Model Extensions**
```yaml
davidson_model_ties:
  probability_i_beats_j: "P(i > j) = Ï€_i / (Ï€_i + Ï€_j + Î½âˆš(Ï€_i Ï€_j))"
  probability_tie: "P(i = j) = Î½âˆš(Ï€_i Ï€_j) / (Ï€_i + Ï€_j + Î½âˆš(Ï€_i Ï€_j))"
  tie_parameter: "Î½ â‰¥ 0"
  
random_effects_extension:
  judge_heterogeneity: "Ï€_ij = Ï€_i Ã— Î³_j"
  where: "Î³_j ~ Gamma(Î±, Î²) represents judge j's scaling"
  
covariate_models:
  strength_regression: "log Ï€_i = Î²â‚€ + Î²â‚X_i1 + ... + Î²_p X_ip"
  interpretation: "Explains strength in terms of object characteristics"
```

### **3.3 Plackett-Luce Model**

#### **Full Ranking Model**
```yaml
ranking_probability:
  full_ranking: "P(ranking) = âˆ_{k=1}^{n-1} Ï€_{r_k} / Î£_{j=k}^n Ï€_{r_j}"
  where: "r_k = object ranked kth"
  
top_k_ranking:
  partial_ranking: "P(top-k) = âˆ_{i=1}^k Ï€_{r_i} / Î£_{j=i}^n Ï€_{r_j}"
  application: "When only top preferences matter"
  
likelihood_function:
  full_data: "LL = Î£Î£ log(Ï€_i / Î£_{jâˆˆS_ik} Ï€_j)"
  where: "S_ik = set of objects available for position k in ranking i"
```

#### **Parameter Estimation and Inference**
```yaml
mm_algorithm:
  update_rule: "Ï€_i^(t+1) = Ï€_i^(t) Ã— Î£_r w_r I(i ranked in r) / Î£_k position_weight(i,k,r)"
  convergence: "Monitor log-likelihood improvement"
  
standard_errors:
  observed_information: "H = -âˆ‚Â²LL/âˆ‚(log Ï€)Â²"
  asymptotic_covariance: "Var(log Ï€) = H^-1"
  
model_selection:
  aic: "AIC = -2LL + 2(n-1)"
  bic: "BIC = -2LL + (n-1)log(N)"
  cross_validation: "Predict held-out rankings"
```

---

## ğŸ¯ Experiment Type 4: A/B Testing

### **4.1 Proportion Tests**

#### **Two-Proportion z-Test**
```yaml
test_statistic:
  z_score: "z = (pÌ‚â‚ - pÌ‚â‚‚) / SE(pÌ‚â‚ - pÌ‚â‚‚)"
  
standard_error_pooled:
  formula: "SE = âˆš(pÌ‚(1-pÌ‚)(1/nâ‚ + 1/nâ‚‚))"
  pooled_proportion: "pÌ‚ = (xâ‚ + xâ‚‚)/(nâ‚ + nâ‚‚)"
  
standard_error_unpooled:
  formula: "SE = âˆš(pÌ‚â‚(1-pÌ‚â‚)/nâ‚ + pÌ‚â‚‚(1-pÌ‚â‚‚)/nâ‚‚)"
  application: "For confidence intervals"
  
continuity_correction:
  yates_correction: "z = (|pÌ‚â‚ - pÌ‚â‚‚| - 0.5(1/nâ‚ + 1/nâ‚‚)) / SE"
  when_to_use: "Small expected frequencies (< 5)"
```

#### **Effect Size Measures**
```yaml
risk_difference:
  formula: "RD = pâ‚ - pâ‚‚"
  confidence_interval: "RD Â± z_Î±/2 Ã— SE_unpooled"
  interpretation: "Absolute difference in success rates"
  
relative_risk:
  formula: "RR = pâ‚ / pâ‚‚"
  log_confidence_interval: "log(RR) Â± z_Î±/2 Ã— SE(log RR)"
  standard_error: "SE(log RR) = âˆš((1-pâ‚)/(nâ‚pâ‚) + (1-pâ‚‚)/(nâ‚‚pâ‚‚))"
  
odds_ratio:
  formula: "OR = (pâ‚/(1-pâ‚)) / (pâ‚‚/(1-pâ‚‚))"
  log_confidence_interval: "log(OR) Â± z_Î±/2 Ã— SE(log OR)"
  standard_error: "SE(log OR) = âˆš(1/a + 1/b + 1/c + 1/d)"
  where: "a,b,c,d are cells of 2Ã—2 contingency table"
  
cohens_h:
  formula: "h = 2(arcsin(âˆšpâ‚) - arcsin(âˆšpâ‚‚))"
  interpretation: "|h| = 0.2 (small), 0.5 (medium), 0.8 (large)"
```

#### **Power Analysis**
```yaml
sample_size_calculation:
  equal_allocation: "n = 2(z_Î±/2 + z_Î²)Â² Ã— [pâ‚(1-pâ‚) + pâ‚‚(1-pâ‚‚)] / (pâ‚-pâ‚‚)Â²"
  unequal_allocation: "nâ‚ = (z_Î±/2 + z_Î²)Â² Ã— [pâ‚(1-pâ‚)/r + pâ‚‚(1-pâ‚‚)] / (pâ‚-pâ‚‚)Â²"
  where: "r = nâ‚/nâ‚‚"
  
arcsine_transformation:
  variance_stabilizing: "n = 2(z_Î±/2 + z_Î²)Â² / hÂ²"
  where: "h = Cohen's h"
  
minimum_detectable_effect:
  formula: "MDE = (z_Î±/2 + z_Î²) Ã— âˆš(2pÌ„(1-pÌ„)/n)"
  where: "pÌ„ = (pâ‚ + pâ‚‚)/2"
```

### **4.2 Continuous Outcome A/B Tests**

#### **Welch's t-Test (Unequal Variances)**
```yaml
test_statistic:
  formula: "t = (xÌ„â‚ - xÌ„â‚‚) / âˆš(sâ‚Â²/nâ‚ + sâ‚‚Â²/nâ‚‚)"
  
satterthwaite_df:
  formula: "df = (sâ‚Â²/nâ‚ + sâ‚‚Â²/nâ‚‚)Â² / [(sâ‚Â²/nâ‚)Â²/(nâ‚-1) + (sâ‚‚Â²/nâ‚‚)Â²/(nâ‚‚-1)]"
  
power_analysis:
  noncentrality: "Î´ = (Î¼â‚ - Î¼â‚‚) / âˆš(Ïƒâ‚Â²/nâ‚ + Ïƒâ‚‚Â²/nâ‚‚)"
  power_calculation: "Use noncentral t-distribution"
```

#### **Mann-Whitney U Test (Non-parametric)**
```yaml
test_statistic:
  u_statistic: "U = nâ‚nâ‚‚ + nâ‚(nâ‚+1)/2 - Râ‚"
  where: "Râ‚ = sum of ranks for group 1"
  
normal_approximation:
  z_score: "z = (U - Î¼áµ¤) / Ïƒáµ¤"
  mean: "Î¼áµ¤ = nâ‚nâ‚‚/2"
  variance: "ÏƒÂ²áµ¤ = nâ‚nâ‚‚(nâ‚+nâ‚‚+1)/12"
  
continuity_correction:
  adjusted_z: "z = (|U - Î¼áµ¤| - 0.5) / Ïƒáµ¤"
  
effect_size:
  probability_superiority: "P(Xâ‚ > Xâ‚‚) = U / (nâ‚ Ã— nâ‚‚)"
  rank_biserial_correlation: "r = 2P(Xâ‚ > Xâ‚‚) - 1"
```

### **4.3 Sequential Testing and Early Stopping**

#### **Group Sequential Design**
```yaml
alpha_spending_functions:
  obrien_fleming: "Î±(t) = 2[1 - Î¦(z_Î±/2/âˆšt)]"
  pocock: "Î±(t) = Î± Ã— log(1 + (e-1)t)"
  where: "t = information fraction (0 < t â‰¤ 1)"
  
  lan_demets: "General class of spending functions"
  power_family: "Î±(t) = Î± Ã— t^Ï"
  
boundary_calculation:
  critical_values: "c_k satisfying Î±-spending at analysis k"
  stopping_boundaries: "Upper and lower bounds for test statistic"
  
sample_size_adjustment:
  information_based: "Calculate required information, convert to sample size"
  maximum_sample_size: "Typically 10-15% larger than fixed design"
```

#### **Bayesian Sequential Testing**
```yaml
posterior_probability:
  beta_binomial: "Success rate ~ Beta(Î± + x, Î² + n - x)"
  normal_normal: "Mean ~ N(Î¼_posterior, ÏƒÂ²_posterior)"
  
stopping_criteria:
  posterior_probability: "P(treatment better | data) > threshold"
  credible_interval: "95% CI excludes null value"
  
expected_sample_size:
  simulation_based: "Average N over posterior predictive distribution"
  analytical_approximation: "For conjugate priors"
```

---

## âš¡ Experiment Type 5: Single Sample Rating

### **5.1 One-Sample Tests**

#### **One-Sample t-Test**
```yaml
test_statistic:
  formula: "t = (xÌ„ - Î¼â‚€) / (s / âˆšn)"
  degrees_of_freedom: "df = n - 1"
  
effect_size:
  cohens_d: "d = (xÌ„ - Î¼â‚€) / s"
  confidence_interval: "d Â± t_Î±/2,df Ã— âˆš((n-1)/Ï‡Â²_1-Î±/2,n-1 + dÂ²/(2n))"
  
power_analysis:
  sample_size: "n = (t_Î±/2,df + t_Î²,df)Â² / dÂ²"
  noncentrality: "Î´ = dâˆšn"
  power: "P(t > t_Î±/2,df | Î´)"
```

#### **Wilcoxon Signed-Rank Test**
```yaml
test_statistic:
  signed_ranks: "Rank |x_i - Î¼â‚€| and sum positive ranks"
  normal_approximation: "z = (W - Î¼_w) / Ïƒ_w"
  mean: "Î¼_w = n(n+1)/4"
  variance: "ÏƒÂ²_w = n(n+1)(2n+1)/24"
  
effect_size:
  probability_superiority: "P(X > Î¼â‚€)"
  matched_pairs_rank_biserial: "r = (W - n(n+1)/4) / (n(n+1)/4)"
```

### **5.2 Confidence Intervals and Precision**

#### **Mean Confidence Intervals**
```yaml
t_interval:
  formula: "xÌ„ Â± t_Î±/2,df Ã— (s / âˆšn)"
  
bootstrap_intervals:
  percentile_method: "Use 2.5th and 97.5th percentiles of bootstrap distribution"
  bias_corrected: "Adjust for bias in bootstrap estimate"
  bca_method: "Bias-corrected and accelerated intervals"
  
precision_planning:
  margin_of_error: "ME = t_Î±/2,df Ã— (s / âˆšn)"
  required_sample_size: "n = (t_Î±/2 Ã— s / ME)Â²"
  pilot_study_approach: "Use pilot s to plan main study"
```

#### **Proportion Confidence Intervals**
```yaml
wald_interval:
  formula: "pÌ‚ Â± z_Î±/2 Ã— âˆš(pÌ‚(1-pÌ‚)/n)"
  limitation: "Poor coverage for extreme proportions"
  
wilson_score_interval:
  formula: "(pÌ‚ + zÂ²/2n Â± zâˆš(pÌ‚(1-pÌ‚)/n + zÂ²/4nÂ²)) / (1 + zÂ²/n)"
  advantage: "Better coverage properties"
  
clopper_pearson_exact:
  based_on: "Beta distribution quantiles"
  conservative: "Always achieves nominal coverage"
  
jeffreys_interval:
  bayesian_approach: "Based on uniform prior (Beta(0.5, 0.5))"
  credible_interval: "Often close to frequentist intervals"
```

### **5.3 Reliability Analysis**

#### **Internal Consistency**
```yaml
cronbachs_alpha:
  formula: "Î± = (k/(k-1)) Ã— (1 - Î£sÂ²áµ¢/sÂ²â‚œ)"
  where:
    k: "number of items"
    sÂ²áµ¢: "variance of item i"
    sÂ²â‚œ: "variance of total score"
    
confidence_interval:
  feldt_method: "Based on F-distribution"
  bootstrap: "Empirical distribution of Î±"
  
interpretation:
  unacceptable: "Î± < 0.60"
  poor: "0.60 â‰¤ Î± < 0.70"
  acceptable: "0.70 â‰¤ Î± < 0.80"
  good: "0.80 â‰¤ Î± < 0.90"
  excellent: "Î± â‰¥ 0.90"
  
mcdonalds_omega:
  formula: "Ï‰ = (Î£Î»áµ¢)Â² / [(Î£Î»áµ¢)Â² + Î£(1-Î»áµ¢Â²)]"
  where: "Î»áµ¢ = factor loading for item i"
  advantage: "More robust to tau-equivalent model violations"
```

#### **Test-Retest Reliability**
```yaml
pearson_correlation:
  formula: "r = Î£(Xâ‚ - XÌ„â‚)(Xâ‚‚ - XÌ„â‚‚) / âˆš[Î£(Xâ‚ - XÌ„â‚)Â²Î£(Xâ‚‚ - XÌ„â‚‚)Â²]"
  
intraclass_correlation:
  two_way_mixed: "ICC(3,1) = (MS_subjects - MS_error) / (MS_subjects + (k-1)MS_error)"
  interpretation: "Same as reliability coefficient"
  
standard_error_measurement:
  formula: "SEM = s Ã— âˆš(1 - reliability)"
  confidence_interval: "X Â± 1.96 Ã— SEM"
  
minimal_detectable_change:
  formula: "MDC = SEM Ã— z_Î±/2 Ã— âˆš2"
  interpretation: "Smallest change exceeding measurement error"
```

---

## ğŸ”¬ Experiment Type 6: Threshold Detection

### **6.1 Psychometric Function Fitting**

#### **Sigmoid Function Forms**
```yaml
logistic_function:
  formula: "Î¨(x) = Î³ + (1 - Î³ - Î»)[1 / (1 + exp(-(x - Î±)/Î²))]"
  parameters:
    Î±: "threshold (50% point after adjusting for guess/lapse)"
    Î²: "slope parameter (related to standard deviation)"
    Î³: "guess rate (lower asymptote)"
    Î»: "lapse rate (1 - upper asymptote)"
    
cumulative_gaussian:
  formula: "Î¨(x) = Î³ + (1 - Î³ - Î»)Î¦((x - Î±)/Î²)"
  where: "Î¦ = cumulative standard normal"
  
weibull_function:
  formula: "Î¨(x) = 1 - (1 - Î³)exp(-((x/Î±)^Î²))"
  application: "Often used in vision research"
```

#### **Maximum Likelihood Estimation**
```yaml
likelihood_function:
  binomial_model: "L = âˆáµ¢ (náµ¢ choose káµ¢) Ã— Î¨(xáµ¢)^káµ¢ Ã— (1 - Î¨(xáµ¢))^(náµ¢-káµ¢)"
  where:
    náµ¢: "number of trials at stimulus level xáµ¢"
    káµ¢: "number of positive responses at xáµ¢"
    
log_likelihood:
  formula: "LL = Î£áµ¢ [káµ¢ log Î¨(xáµ¢) + (náµ¢ - káµ¢) log(1 - Î¨(xáµ¢))]"
  
parameter_estimation:
  optimization: "Maximize LL using numerical methods"
  initial_values: "Use method of moments or grid search"
  constraints: "0 â‰¤ Î³,Î» â‰¤ 1, Î±,Î² > 0"
```

#### **Goodness of Fit and Inference**
```yaml
deviance_test:
  deviance: "D = 2(LL_saturated - LL_fitted)"
  distribution: "D ~ Ï‡Â²(df) under Hâ‚€"
  degrees_of_freedom: "df = number of stimulus levels - number of parameters"
  
confidence_intervals:
  profile_likelihood: "Values where -2Î”LL â‰¤ Ï‡Â²_Î±,1"
  bootstrap: "Empirical distribution from resampling"
  delta_method: "First-order approximation using Fisher information"
  
bias_correction:
  bootstrap_bias: "Bias = mean(Î¸Ì‚*) - Î¸Ì‚"
  bias_corrected_estimate: "Î¸Ì‚_BC = Î¸Ì‚ - bias"
```

### **6.2 Adaptive Testing Procedures**

#### **Up-Down Methods**
```yaml
simple_staircase:
  rule: "Increase intensity after incorrect, decrease after correct"
  convergence_point: "50% correct (for 2AFC)"
  step_size: "Fixed or geometric progression"
  
weighted_up_down:
  levitt_rules:
    70_7_percent: "2-down-1-up rule"
    79_4_percent: "3-down-1-up rule"
    91_0_percent: "4-down-1-up rule"
  
  convergence_probability: "P = (step_down/step_up)^(1/k)"
  where: "k = number of consecutive correct for step down"
  
stopping_criteria:
  fixed_reversals: "Stop after predetermined number of reversals"
  standard_error: "Stop when SE(threshold) < criterion"
  maximum_trials: "Safety stop to prevent infinite runs"
```

#### **Adaptive Procedures Analysis**
```yaml
threshold_estimation:
  reversal_average: "Î¸Ì‚ = mean of last N reversals"
  weighted_average: "Higher weight to later reversals"
  maximum_likelihood: "Fit psychometric function to all data"
  
standard_error:
  empirical_se: "SE = SD(reversals) / âˆšN"
  theoretical_se: "Based on psychometric function slope"
  
efficiency_measures:
  trial_efficiency: "Compare to method of constant stimuli"
  information_efficiency: "Fisher information per trial"
```

### **6.3 Bayesian Adaptive Methods**

#### **QUEST (Quick Estimation by Sequential Testing)**
```yaml
prior_distribution:
  threshold_prior: "p(Î±) ~ N(Î¼_prior, ÏƒÂ²_prior)"
  slope_prior: "p(Î²) ~ Gamma(a, b)"
  
posterior_update:
  bayes_rule: "p(Î±|data) âˆ p(data|Î±) Ã— p(Î±)"
  sequential_update: "Update after each trial"
  
stimulus_selection:
  information_criterion: "Maximize expected information gain"
  entropy_criterion: "Minimize expected posterior entropy"
  
threshold_estimate:
  posterior_mode: "Î±Ì‚ = argmax p(Î±|data)"
  posterior_mean: "Î±Ì‚ = E[Î±|data]"
  credible_interval: "95% highest posterior density"
```

#### **Implementation Formulas**
```python
# Python implementation for psychometric function fitting
import numpy as np
from scipy import optimize, stats
from scipy.special import erf

def logistic_psychometric(x, alpha, beta, gamma=0.0, lambda_val=0.0):
    """
    Logistic psychometric function
    x: stimulus intensity
    alpha: threshold parameter
    beta: slope parameter  
    gamma: guess rate
    lambda_val: lapse rate
    """
    return gamma + (1 - gamma - lambda_val) / (1 + np.exp(-(x - alpha) / beta))

def gaussian_psychometric(x, alpha, beta, gamma=0.0, lambda_val=0.0):
    """
    Cumulative Gaussian psychometric function
    """
    z = (x - alpha) / beta
    return gamma + (1 - gamma - lambda_val) * 0.5 * (1 + erf(z / np.sqrt(2)))

def neg_log_likelihood(params, x, k, n, func_type='logistic'):
    """
    Negative log-likelihood for psychometric function
    params: [alpha, beta, gamma, lambda_val]
    x: stimulus intensities
    k: number of positive responses
    n: number of trials
    """
    alpha, beta, gamma, lambda_val = params
    
    # Ensure valid parameter ranges
    if beta <= 0 or gamma < 0 or gamma > 1 or lambda_val < 0 or lambda_val > 1:
        return np.inf
    if gamma + lambda_val >= 1:
        return np.inf
    
    if func_type == 'logistic':
        p = logistic_psychometric(x, alpha, beta, gamma, lambda_val)
    elif func_type == 'gaussian':
        p = gaussian_psychometric(x, alpha, beta, gamma, lambda_val)
    
    # Avoid log(0) problems
    p = np.clip(p, 1e-10, 1 - 1e-10)
    
    # Binomial log-likelihood
    ll = np.sum(k * np.log(p) + (n - k) * np.log(1 - p))
    return -ll

def fit_psychometric_function(x, k, n, func_type='logistic', initial_guess=None):
    """
    Fit psychometric function to data using maximum likelihood
    Returns parameter estimates and confidence intervals
    """
    if initial_guess is None:
        # Simple initial guess based on data
        alpha_init = x[np.argmin(np.abs(k/n - 0.5))]
        beta_init = (np.max(x) - np.min(x)) / 4
        initial_guess = [alpha_init, beta_init, 0.0, 0.0]
    
    # Optimize
    result = optimize.minimize(
        neg_log_likelihood,
        initial_guess,
        args=(x, k, n, func_type),
        method='L-BFGS-B',
        bounds=[(-np.inf, np.inf), (0.001, np.inf), (0, 0.5), (0, 0.5)]
    )
    
    # Calculate confidence intervals using Hessian
    try:
        hessian = result.hess_inv.todense()
        se = np.sqrt(np.diag(hessian))
        ci_lower = result.x - 1.96 * se
        ci_upper = result.x + 1.96 * se
    except:
        # If Hessian calculation fails, use bootstrap
        ci_lower = ci_upper = None
    
    return {
        'parameters': result.x,
        'success': result.success,
        'log_likelihood': -result.fun,
        'ci_lower': ci_lower,
        'ci_upper': ci_upper
    }

def deviance_goodness_of_fit(x, k, n, fitted_params, func_type='logistic'):
    """
    Calculate deviance statistic for goodness of fit test
    """
    alpha, beta, gamma, lambda_val = fitted_params
    
    if func_type == 'logistic':
        p_fit = logistic_psychometric(x, alpha, beta, gamma, lambda_val)
    elif func_type == 'gaussian':
        p_fit = gaussian_psychometric(x, alpha, beta, gamma, lambda_val)
    
    # Saturated model likelihood (perfect fit)
    p_obs = k / n
    p_obs = np.clip(p_obs, 1e-10, 1 - 1e-10)  # Avoid log(0)
    
    ll_saturated = np.sum(k * np.log(p_obs) + (n - k) * np.log(1 - p_obs))
    ll_fitted = np.sum(k * np.log(p_fit) + (n - k) * np.log(1 - p_fit))
    
    deviance = 2 * (ll_saturated - ll_fitted)
    df = len(x) - len(fitted_params)
    p_value = 1 - stats.chi2.cdf(deviance, df)
    
    return {'deviance': deviance, 'df': df, 'p_value': p_value}
```

---

## ğŸ“Š Implementation Framework

### **Statistical Computing Infrastructure**
```yaml
required_libraries:
  python:
    core: ["numpy", "scipy", "pandas", "matplotlib"]
    statistics: ["statsmodels", "pingouin", "scikit-learn"]
    power_analysis: ["power", "pwr"]
    psychometrics: ["psignifit", "psychopy"]
    
  r_packages:
    core: ["base", "stats", "MASS"]
    power_analysis: ["pwr", "WebPower", "PowerTOST"] 
    mixed_models: ["lme4", "nlme", "lmerTest"]
    psychometrics: ["psycho", "psych", "modelfree"]
    
validation_testing:
  unit_tests: "Test each formula against known results"
  integration_tests: "Test complete analysis workflows"
  benchmark_datasets: "Validate against published examples"
  edge_case_handling: "Test boundary conditions and error states"
```

### **Quality Assurance Protocol**
```yaml
accuracy_validation:
  cross_reference: "Compare results across different software implementations"
  literature_validation: "Verify against published examples and datasets"
  expert_review: "Statistical expert validation of all implementations"
  
error_handling:
  numerical_stability: "Robust handling of edge cases and numerical issues"
  assumption_checking: "Automatic testing of statistical assumptions"
  warning_systems: "Alert users to potential problems"
  
documentation_standards:
  mathematical_notation: "Clear explanation of all formulas and parameters"
  implementation_notes: "Software-specific considerations and limitations"
  example_usage: "Complete worked examples for each method"
```

---

*Statistical Equations Library Version: 1.0*  
*Complete Mathematical Foundation for Agent Implementation*  
*Validated Against Academic Standards and Best Practices*